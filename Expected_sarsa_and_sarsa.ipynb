{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8kdEPNmFOCr"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "T9cAvA0GLkXh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys    \n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wMAd6qTASn9u"
   },
   "outputs": [],
   "source": [
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\") \n",
    "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\")\n",
    "OBSTACLES = (\"#\")\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "rand_move_probability = 0\n",
    "\n",
    "class World:  \n",
    "  def __init__(self, width, height):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.grid = np.full((width, height), ' ', dtype='U1')\n",
    "  \n",
    "  def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "    \"\"\"\n",
    "    Create an obstacle in either a single cell or rectangle.\n",
    "    \"\"\"\n",
    "    if end_x == None: end_x = start_x\n",
    "    if end_y == None: end_y = start_y\n",
    "    \n",
    "    self.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "  def add_reward(self, x, y, reward):\n",
    "    assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "    self.grid[x, y] = reward\n",
    "\n",
    "  def add_terminal(self, x, y, terminal):\n",
    "    assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "    self.grid[x, y] = terminal\n",
    "\n",
    "  def is_obstacle(self, x, y):\n",
    "    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "      return True\n",
    "    else:\n",
    "      return self.grid[x ,y] in OBSTACLES \n",
    "\n",
    "  def is_terminal(self, x, y):\n",
    "    return self.grid[x ,y] in TERMINALS\n",
    "\n",
    "  def get_reward(self, x, y):\n",
    "    \"\"\" \n",
    "    Return the reward associated with a given location\n",
    "    \"\"\" \n",
    "    return REWARDS[self.grid[x, y]]\n",
    "\n",
    "  def get_next_state(self, current_state, action):\n",
    "    \"\"\"\n",
    "    Get the next state given a current state and an action. The outcome can be\n",
    "    stochastic  where rand_move_probability determines the probability of \n",
    "    ignoring the action and performing a random move.\n",
    "    \"\"\"    \n",
    "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "    x, y = current_state \n",
    "    \n",
    "    if self.grid[x, y] in TERMINALS:\n",
    "      return None\n",
    "\n",
    "    if np.random.rand() < rand_move_probability:\n",
    "      action = np.random.choice(ACTIONS)\n",
    "\n",
    "    if action == \"up\":      y -= 1\n",
    "    elif action == \"down\":  y += 1\n",
    "    elif action == \"left\":  x -= 1\n",
    "    elif action == \"right\": x += 1\n",
    "\n",
    "    return (x, y) if not self.is_obstacle(x, y) else current_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wEIxyiw1J1fd"
   },
   "outputs": [],
   "source": [
    "world = World(8, 8)\n",
    "world.add_terminal(7, 7, \"+\")\n",
    "\n",
    "Q = {}\n",
    "for x in range(world.width):\n",
    "    for y in range(world.height):\n",
    "        for action in ACTIONS:\n",
    "            Q[((x, y), action)] = 0.0\n",
    "\n",
    "            \n",
    "def epsilon_greedy_policy(state, epsilon=0.1):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(ACTIONS) \n",
    "    else:\n",
    "        q_values = [Q[(state, action)] for action in ACTIONS]\n",
    "        max_q = max(q_values)\n",
    "        max_actions = [action for action, q in zip(ACTIONS, q_values) if q == max_q]\n",
    "        return random.choice(max_actions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def expected_sarsa(world, alpha=0.99, gamma=0.9, epsilon=0.1, episodes=1000):\n",
    "    for _ in range(episodes):\n",
    "        state = (random.randint(0, world.width - 1), random.randint(0, world.height - 1))\n",
    "        action = epsilon_greedy_policy(state, epsilon)\n",
    "\n",
    "        while not world.is_terminal(*state):\n",
    "            next_state = world.get_next_state(state, action)\n",
    "            reward = world.get_reward(*next_state)\n",
    "\n",
    "            expected_value = sum(\n",
    "            [Q[(next_state, a)] * (epsilon / len(ACTIONS)) for a in ACTIONS]\n",
    "            )\n",
    "            expected_value += Q[(next_state, action)] * (1 - epsilon + epsilon / len(ACTIONS))\n",
    "\n",
    "            Q[(state, action)] += alpha * (reward + gamma * expected_value - Q[(state, action)])\n",
    "\n",
    "            state = next_state\n",
    "            action = epsilon_greedy_policy(state, epsilon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_J7MOXnFgXZI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q((7, 6), up): 0.5417716272790034 position above terminal\n",
      "Q((6, 7), up): 0.21677358897261062 position to the left of terminal\n",
      "Q((7, 6), down): 10.0 position above terminal\n",
      "Q((6, 7), down): 1.148172296772975 position to the left of terminal\n",
      "Q((7, 6), left): 0.10592751836884097 position above terminal\n",
      "Q((6, 7), left): 0.4347915747057224 position to the left of terminal\n",
      "Q((7, 6), right): 3.1617957362950757 position above terminal\n",
      "Q((6, 7), right): 10.0 position to the left of terminal\n"
     ]
    }
   ],
   "source": [
    "expected_sarsa(world, alpha=0.99, gamma=0.99, epsilon=0.1, episodes=1000)\n",
    "\n",
    "for action in ACTIONS:\n",
    "    print(f\"Q((7, 6), {action}): {Q[((7, 6), action)]}\", \"position above terminal\")\n",
    "    print(f\"Q((6, 7), {action}): {Q[((6, 7), action)]}\", \"position to the left of terminal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dz1Sqh95hoyC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_move_probability = 0.0\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "def sarsa(world, alpha, gamma=0.9, epsilon=0.1, episodes=1000):\n",
    "    for _ in range(episodes):\n",
    "        state = (0,0)\n",
    "        action = epsilon_greedy_policy(state, epsilon)\n",
    "\n",
    "        while not world.is_terminal(*state):\n",
    "            next_state = world.get_next_state(state, action)\n",
    "            reward = world.get_reward(*next_state)\n",
    "            \n",
    "            next_action = epsilon_greedy_policy(next_state, epsilon)\n",
    "\n",
    "            Q[(state, action)] += alpha * (reward + gamma * Q[(next_state, next_action)] - Q[(state, action)])\n",
    "            state = next_state\n",
    "            action = next_action\n",
    " \n",
    "def measure_steps_to_goal(world):\n",
    "    steps = 0\n",
    "    state = (0, 0)\n",
    "    while not world.is_terminal(*state):\n",
    "        action = epsilon_greedy_policy(state, epsilon=0.1)\n",
    "        next_state = world.get_next_state(state, action)\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    return steps\n",
    "\n",
    "\n",
    "def run_experiment(method, alpha, episodes=1000, runs=100):\n",
    "    results = []\n",
    "    for _ in range(runs):\n",
    "        world = World(8, 8)\n",
    "        world.add_terminal(7, 7, \"+\")\n",
    "        \n",
    "        if method == \"SARSA\":\n",
    "            sarsa(world, alpha, gamma=0.9, epsilon=0.1, episodes=episodes)\n",
    "        elif method == \"Expected SARSA\":\n",
    "            expected_sarsa(world, alpha, gamma=0.9, epsilon=0.1, episodes=episodes)\n",
    "        \n",
    "        steps = measure_steps_to_goal(world)\n",
    "        results.append(steps)\n",
    "    \n",
    "    return sum(results) / len(results) \n",
    "\n",
    "alphas = [0.1, 0.5, 0.99]\n",
    "sarsa_results = [run_experiment(\"SARSA\", alpha) for alpha in alphas]\n",
    "expected_sarsa_result = run_experiment(\"Expected SARSA\", alpha=0.99)\n",
    "\n",
    "print(\"SARSA Results (Different Alphas, 0.1, 0.5, 0.99):\", sarsa_results)\n",
    "print(\"Expected SARSA Result:\", expected_sarsa_result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
