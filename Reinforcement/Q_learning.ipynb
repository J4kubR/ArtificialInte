{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8kdEPNmFOCr"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "T9cAvA0GLkXh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys         \n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wMAd6qTASn9u"
   },
   "outputs": [],
   "source": [
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\") \n",
    "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\", \"#\")\n",
    "OBSTACLES = (\"#\")\n",
    "\n",
    "gamma = 1\n",
    "rand_move_probability = 0\n",
    "\n",
    "class World:  \n",
    "  def __init__(self, width, height):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.grid = np.full((width, height), ' ', dtype='U1')\n",
    "  \n",
    "  def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "    \"\"\"\n",
    "    Create an obstacle in either a single cell or rectangle.\n",
    "    \"\"\"\n",
    "    if end_x == None: end_x = start_x\n",
    "    if end_y == None: end_y = start_y\n",
    "    \n",
    "    self.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "  def add_reward(self, x, y, reward):\n",
    "    assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "    self.grid[x, y] = reward\n",
    "\n",
    "  def add_terminal(self, x, y, terminal):\n",
    "    assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "    self.grid[x, y] = terminal\n",
    "\n",
    "  def is_obstacle(self, x, y):\n",
    "    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "      return True\n",
    "    else:\n",
    "      return self.grid[x ,y] in OBSTACLES \n",
    "\n",
    "  def is_terminal(self, x, y):\n",
    "    return self.grid[x ,y] in TERMINALS\n",
    "\n",
    "  def get_reward(self, x, y):\n",
    "    \"\"\" \n",
    "    Return the reward associated with a given location\n",
    "    \"\"\" \n",
    "    return REWARDS[self.grid[x, y]]\n",
    "\n",
    "  def get_next_state(self, current_state, action):\n",
    "    \"\"\"\n",
    "    Get the next state given a current state and an action. The outcome can be\n",
    "    stochastic  where rand_move_probability determines the probability of \n",
    "    ignoring the action and performing a random move.\n",
    "    \"\"\"    \n",
    "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "    \n",
    "    x, y = current_state \n",
    "  \n",
    "    if self.grid[x, y] in TERMINALS:\n",
    "      return None\n",
    "\n",
    "    if np.random.rand() < rand_move_probability:\n",
    "      action = np.random.choice(ACTIONS)\n",
    "\n",
    "    if action == \"up\":      y -= 1\n",
    "    elif action == \"down\":  y += 1\n",
    "    elif action == \"left\":  x -= 1\n",
    "    elif action == \"right\": x += 1\n",
    "\n",
    "    return (x, y) if not self.is_obstacle(x, y) else current_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEIxyiw1J1fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' '#' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' '+']]\n",
      "\n",
      "[['-' 'R' 'R' 'D' 'D']\n",
      " ['D' 'D' 'R' 'D' 'D']\n",
      " ['R' 'D' '#' 'D' 'D']\n",
      " ['R' 'D' 'D' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' '+']]\n",
      "((0, 1), {'up': -9.999999999305235, 'down': 5.314409999999974, 'left': 4.782958966220196, 'right': 5.3143897448276505})\n",
      "((0, 2), {'up': 4.782968999986369, 'down': 5.904899999699186, 'left': 5.314409999973163, 'right': 5.904899999999975})\n",
      "((0, 3), {'up': 5.314409999999974, 'down': 6.560981470767228, 'left': 5.904888414796627, 'right': 6.560999999999977})\n",
      "((0, 4), {'up': 5.90489668749574, 'down': 6.560584033944391, 'left': 6.560993915514217, 'right': 7.289999999999978})\n",
      "((1, 0), {'up': 4.782968964917476, 'down': 5.314407293956412, 'left': -9.999999999999993, 'right': 5.314409999999974})\n",
      "((1, 1), {'up': 4.782968999999973, 'down': 5.904899999999975, 'left': 4.782968277996572, 'right': 5.904898167663782})\n",
      "((1, 2), {'up': 5.314409999999974, 'down': 6.560999999999977, 'left': 5.314409999999974, 'right': 5.904899999999975})\n",
      "((1, 3), {'up': 5.904899999999975, 'down': 7.289999999999978, 'left': 5.904899999999975, 'right': 7.289999999999978})\n",
      "((1, 4), {'up': 6.560999999999977, 'down': 7.289999999999978, 'left': 6.560999999999977, 'right': 8.09999999999998})\n",
      "((2, 0), {'up': 5.314409999999965, 'down': 5.904899999996856, 'left': 4.78296899999996, 'right': 5.904899999999975})\n",
      "((2, 1), {'up': 5.314409999999974, 'down': 5.904899562069521, 'left': 5.3144076968585825, 'right': 6.560999999999977})\n",
      "((2, 3), {'up': 7.289999999999978, 'down': 8.09999999999998, 'left': 6.560999999999977, 'right': 8.09999999999998})\n",
      "((2, 4), {'up': 7.289999999999978, 'down': 8.09999999999998, 'left': 7.289999999999978, 'right': 8.999999999999986})\n",
      "((3, 0), {'up': 5.904899999999975, 'down': 6.560999999999977, 'left': 5.314409999999974, 'right': 6.560999999999977})\n",
      "((3, 1), {'up': 5.904899999999975, 'down': 7.289999999999978, 'left': 5.904899999999946, 'right': 7.289999999999978})\n",
      "((3, 2), {'up': 6.560999999999977, 'down': 8.09999999999998, 'left': 7.289999999999978, 'right': 8.09999999999998})\n",
      "((3, 3), {'up': 7.289999999999978, 'down': 8.999999999999986, 'left': 7.289999999999978, 'right': 8.999999999999986})\n",
      "((3, 4), {'up': 8.09999999999998, 'down': 8.999999999999986, 'left': 8.09999999999998, 'right': 9.999999999999993})\n",
      "((4, 0), {'up': 6.560999999999977, 'down': 7.289999999999978, 'left': 5.904899999999975, 'right': 6.560999999999977})\n",
      "((4, 1), {'up': 6.560999999999977, 'down': 8.09999999999998, 'left': 6.560999999999977, 'right': 7.289999999999978})\n",
      "((4, 2), {'up': 7.289999999999978, 'down': 8.999999999999986, 'left': 7.289999999999978, 'right': 8.09999999999998})\n",
      "((4, 3), {'up': 8.09999999999998, 'down': 9.999999999999993, 'left': 8.09999999999998, 'right': 8.999999999999986})\n"
     ]
    }
   ],
   "source": [
    "def q_learning(world, alpha=0.1, epsilon=0.1, gamma=1.0, episodes=1000):\n",
    "\n",
    "    Q = {}\n",
    "    for x in range(world.width):\n",
    "        for y in range(world.height):\n",
    "            if world.grid[x, y] not in TERMINALS: \n",
    "                Q[(x, y)] = {action: 0.0 for action in ACTIONS}\n",
    "\n",
    "    def choose_action(state):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(ACTIONS)\n",
    "        else:\n",
    "            q_values = Q[state]\n",
    "            return max(q_values, key=q_values.get)\n",
    "\n",
    "    def update_Q(state, action, reward, next_state):\n",
    "        if next_state is None or world.is_terminal(next_state[0], next_state[1]):\n",
    "            next_q_value = 0\n",
    "        else:\n",
    "            next_q_value = max(Q[next_state].values())\n",
    "        Q[state][action] += alpha * (reward + gamma * next_q_value - Q[state][action])\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        start_x = np.random.randint(world.width)\n",
    "        start_y = np.random.randint(world.height)\n",
    "        while world.is_terminal(start_x, start_y):\n",
    "            start_x = np.random.randint(world.width)\n",
    "            start_y = np.random.randint(world.height)\n",
    "\n",
    "        state = (start_x, start_y)\n",
    "\n",
    "        while not world.is_terminal(state[0], state[1]):\n",
    "            action = choose_action(state)\n",
    "            next_state = world.get_next_state(state, action)\n",
    "            reward = world.get_reward(*next_state)\n",
    "\n",
    "            update_Q(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return Q\n",
    "\n",
    "world = World(5, 5) \n",
    "world.add_obstacle(2, 2)  \n",
    "world.add_terminal(4, 4, '+')\n",
    "world.add_terminal(0, 0, '-')\n",
    "\n",
    "Q = q_learning(world, alpha=0.1, epsilon=0.1, gamma=0.9, episodes=100000)\n",
    "\n",
    "print(world.grid.T)\n",
    "def generate_policy_grid(world, Q):\n",
    "    \"\"\"\n",
    "    Generate a grid showing the best action for each state based on the Q-values.\n",
    "    \n",
    "    Parameters:\n",
    "    - world: the World object representing the environment\n",
    "    - Q: the learned Q-value table\n",
    "    \n",
    "    Returns:\n",
    "    - policy_grid: a grid with 'L', 'R', 'U', 'D' for each state, indicating the best action\n",
    "    \"\"\"\n",
    "    policy_grid = np.full((world.width, world.height), ' ', dtype='U1')\n",
    "    \n",
    "    for x in range(world.width):\n",
    "        for y in range(world.height):\n",
    "            if world.is_terminal(x, y):\n",
    "                policy_grid[x, y] = world.grid[x, y]\n",
    "            else:\n",
    "                state = (x, y)\n",
    "                best_action = max(Q[state], key=Q[state].get) \n",
    "                \n",
    "                if best_action == \"left\":\n",
    "                    policy_grid[x, y] = 'L'\n",
    "                elif best_action == \"right\":\n",
    "                    policy_grid[x, y] = 'R'\n",
    "                elif best_action == \"up\":\n",
    "                    policy_grid[x, y] = 'U'\n",
    "                elif best_action == \"down\":\n",
    "                    policy_grid[x, y] = 'D'\n",
    "    \n",
    "    return policy_grid\n",
    "\n",
    "policy_grid = generate_policy_grid(world, Q)\n",
    "print()\n",
    "print(policy_grid.T)\n",
    "for item in Q.items():\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
