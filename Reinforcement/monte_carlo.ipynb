{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8kdEPNmFOCr"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "T9cAvA0GLkXh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys       \n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wMAd6qTASn9u"
   },
   "outputs": [],
   "source": [
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "\n",
    "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\")\n",
    "OBSTACLES = (\"#\")\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "rand_move_probability = 0\n",
    "\n",
    "class World:\n",
    "  def __init__(self, width, height):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    # Create an empty world where the agent can move to all cells\n",
    "    self.grid = np.full((width, height), ' ', dtype='U1')\n",
    "\n",
    "  def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "    \"\"\"\n",
    "    Create an obstacle in either a single cell or rectangle.\n",
    "    \"\"\"\n",
    "    if end_x == None: end_x = start_x\n",
    "    if end_y == None: end_y = start_y\n",
    "\n",
    "    self.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "  def add_reward(self, x, y, reward):\n",
    "    assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "    self.grid[x, y] = reward\n",
    "\n",
    "  def add_terminal(self, x, y, terminal):\n",
    "    assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "    self.grid[x, y] = terminal\n",
    "\n",
    "  def is_obstacle(self, x, y):\n",
    "    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "      return True\n",
    "    else:\n",
    "      return self.grid[x ,y] in OBSTACLES\n",
    "\n",
    "  def is_terminal(self, x, y):\n",
    "    return self.grid[x ,y] in TERMINALS\n",
    "\n",
    "  def get_reward(self, x, y):\n",
    "    \"\"\"\n",
    "    Return the reward associated with a given location\n",
    "    \"\"\"\n",
    "    return REWARDS[self.grid[x, y]]\n",
    "\n",
    "  def get_next_state(self, current_state, action):\n",
    "    \"\"\"\n",
    "    Get the next state given a current state and an action. The outcome can be\n",
    "    stochastic  where rand_move_probability determines the probability of\n",
    "    ignoring the action and performing a random move.\n",
    "    \"\"\"\n",
    "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "    x, y = current_state\n",
    "\n",
    "    if self.grid[x, y] in TERMINALS:\n",
    "      return None\n",
    "\n",
    "    if np.random.rand() < rand_move_probability:\n",
    "      action = np.random.choice(ACTIONS)\n",
    "\n",
    "    if action == \"up\":      y -= 1\n",
    "    elif action == \"down\":  y += 1\n",
    "    elif action == \"left\":  x -= 1\n",
    "    elif action == \"right\": x += 1\n",
    "\n",
    "    return (x, y) if not self.is_obstacle(x, y) else current_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dL-XC6-aN6ss",
    "outputId": "03a82fc1-d2b9-4128-de6a-a2a7fc67551e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' '+']]\n"
     ]
    }
   ],
   "source": [
    "world = World(4, 4)\n",
    "\n",
    "world.add_terminal(3, 3, \"+\")\n",
    "\n",
    "def equiprobable_random_policy(x, y):\n",
    "  return { k:1/len(ACTIONS) for k in ACTIONS }\n",
    "\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "51rP-8eH4w1V"
   },
   "outputs": [],
   "source": [
    "def generate_episode(world, policy, start_state):\n",
    "    current_state = start_state\n",
    "    episode = []\n",
    "    while not world.is_terminal(*current_state):\n",
    "        possible_actions = policy(*current_state)\n",
    "\n",
    "        action = random.choices(population=list(possible_actions.keys()),\n",
    "                                weights=possible_actions.values(), k=1)\n",
    "\n",
    "        next_state = world.get_next_state(current_state, action[0])\n",
    "        reward = world.get_reward(*next_state)\n",
    "        episode.append([current_state, action[0], reward])\n",
    "        current_state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PsyPGgMt2skf",
    "outputId": "c121ddf6-479e-4317-8c69-3c093ded5a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0:\n",
      "      State Action  Reward\n",
      "0    (0, 0)     up       0\n",
      "1    (0, 0)     up       0\n",
      "2    (0, 0)   down       0\n",
      "3    (0, 1)     up       0\n",
      "4    (0, 0)   left       0\n",
      "..      ...    ...     ...\n",
      "131  (0, 2)  right       0\n",
      "132  (1, 2)  right       0\n",
      "133  (2, 2)  right       0\n",
      "134  (3, 2)  right       0\n",
      "135  (3, 2)   down      10\n",
      "\n",
      "[136 rows x 3 columns]\n",
      "\n",
      "Episode 1:\n",
      "     State Action  Reward\n",
      "0   (0, 0)     up       0\n",
      "1   (0, 0)  right       0\n",
      "2   (1, 0)   down       0\n",
      "3   (1, 1)   left       0\n",
      "4   (0, 1)     up       0\n",
      "5   (0, 0)     up       0\n",
      "6   (0, 0)   down       0\n",
      "7   (0, 1)     up       0\n",
      "8   (0, 0)  right       0\n",
      "9   (1, 0)   left       0\n",
      "10  (0, 0)  right       0\n",
      "11  (1, 0)  right       0\n",
      "12  (2, 0)  right       0\n",
      "13  (3, 0)  right       0\n",
      "14  (3, 0)     up       0\n",
      "15  (3, 0)  right       0\n",
      "16  (3, 0)     up       0\n",
      "17  (3, 0)   left       0\n",
      "18  (2, 0)   down       0\n",
      "19  (2, 1)     up       0\n",
      "20  (2, 0)  right       0\n",
      "21  (3, 0)   down       0\n",
      "22  (3, 1)     up       0\n",
      "23  (3, 0)   down       0\n",
      "24  (3, 1)   left       0\n",
      "25  (2, 1)  right       0\n",
      "26  (3, 1)  right       0\n",
      "27  (3, 1)  right       0\n",
      "28  (3, 1)  right       0\n",
      "29  (3, 1)  right       0\n",
      "30  (3, 1)  right       0\n",
      "31  (3, 1)  right       0\n",
      "32  (3, 1)   down       0\n",
      "33  (3, 2)   down      10\n",
      "\n",
      "Episode 2:\n",
      "    State Action  Reward\n",
      "0  (0, 0)  right       0\n",
      "1  (1, 0)   down       0\n",
      "2  (1, 1)  right       0\n",
      "3  (2, 1)   down       0\n",
      "4  (2, 2)   down       0\n",
      "5  (2, 3)  right      10\n",
      "\n",
      "Episode 3:\n",
      "     State Action  Reward\n",
      "0   (0, 0)   left       0\n",
      "1   (0, 0)   down       0\n",
      "2   (0, 1)   left       0\n",
      "3   (0, 1)  right       0\n",
      "4   (1, 1)   left       0\n",
      "..     ...    ...     ...\n",
      "87  (3, 2)     up       0\n",
      "88  (3, 1)   left       0\n",
      "89  (2, 1)   down       0\n",
      "90  (2, 2)  right       0\n",
      "91  (3, 2)   down      10\n",
      "\n",
      "[92 rows x 3 columns]\n",
      "\n",
      "Episode 4:\n",
      "      State Action  Reward\n",
      "0    (0, 0)   left       0\n",
      "1    (0, 0)   down       0\n",
      "2    (0, 1)   down       0\n",
      "3    (0, 2)     up       0\n",
      "4    (0, 1)  right       0\n",
      "..      ...    ...     ...\n",
      "104  (2, 1)   down       0\n",
      "105  (2, 2)  right       0\n",
      "106  (3, 2)  right       0\n",
      "107  (3, 2)  right       0\n",
      "108  (3, 2)   down      10\n",
      "\n",
      "[109 rows x 3 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Episode {i}:\")\n",
    "    episode = generate_episode(world, equiprobable_random_policy, (0, 0))\n",
    "    print(pd.DataFrame(episode, columns=[\"State\", \"Action\", \"Reward\"]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fU4gAj-OThu9"
   },
   "source": [
    "Implementation of an on-policy Monte Carlo-based control with an $\\epsilon$-soft policy for estimation of action values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "fYJpKFy82PId"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State (0, 0): \n",
      "Q-values: {'up': 5.043828780198102, 'down': 5.58181546268142, 'left': 5.043727515901997, 'right': 5.644629285297216}\n",
      "Best action: right\n",
      "Action probabilities: {'up': 0.025, 'down': 0.025, 'left': 0.025, 'right': 0.925}\n",
      "\n",
      "State (0, 1): \n",
      "Q-values: {'up': 5.012854492419005, 'down': 6.287415537474025, 'left': 5.6430082690783765, 'right': 6.108725594039598}\n",
      "Best action: down\n",
      "Action probabilities: {'up': 0.025, 'down': 0.925, 'left': 0.025, 'right': 0.025}\n",
      "\n",
      "State (0, 2): \n",
      "Q-values: {'up': 5.631122419342105, 'down': 7.045635377565554, 'left': 6.320587592647055, 'right': 7.045537989754582}\n",
      "Best action: down\n",
      "Action probabilities: {'up': 0.025, 'down': 0.925, 'left': 0.025, 'right': 0.025}\n",
      "\n",
      "State (0, 3): \n",
      "Q-values: {'up': 6.1187443514532855, 'down': 6.850741616110698, 'left': 6.894616014495413, 'right': 7.913054537543168}\n",
      "Best action: right\n",
      "Action probabilities: {'up': 0.025, 'down': 0.025, 'left': 0.025, 'right': 0.925}\n",
      "\n",
      "State (1, 0): \n",
      "Q-values: {'up': 5.646452979074558, 'down': 6.292109989289839, 'left': 5.056979689171445, 'right': 6.321436339279972}\n",
      "Best action: right\n",
      "Action probabilities: {'up': 0.025, 'down': 0.025, 'left': 0.025, 'right': 0.925}\n",
      "\n",
      "State (1, 1): \n",
      "Q-values: {'up': 5.586570152158877, 'down': 7.039818456617638, 'left': 5.555296199264396, 'right': 7.08320135318784}\n",
      "Best action: right\n",
      "Action probabilities: {'up': 0.025, 'down': 0.025, 'left': 0.025, 'right': 0.925}\n",
      "\n",
      "State (1, 2): \n",
      "Q-values: {'up': 6.263332476923072, 'down': 7.937072763300727, 'left': 6.255233099999995, 'right': 7.850929744285722}\n",
      "Best action: down\n",
      "Action probabilities: {'up': 0.025, 'down': 0.925, 'left': 0.025, 'right': 0.025}\n",
      "\n",
      "State (1, 3): \n",
      "Q-values: {'up': 7.068904714285699, 'down': 7.857852323159175, 'left': 6.93282105693856, 'right': 8.925722651558676}\n",
      "Best action: right\n",
      "Action probabilities: {'up': 0.025, 'down': 0.025, 'left': 0.025, 'right': 0.925}\n",
      "\n",
      "State (2, 0): \n",
      "Q-values: {'up': 6.302621950583621, 'down': 7.08235847608332, 'left': 5.643921958529537, 'right': 6.940292974029185}\n",
      "Best action: down\n",
      "Action probabilities: {'up': 0.025, 'down': 0.925, 'left': 0.025, 'right': 0.025}\n",
      "\n",
      "State (2, 1): \n",
      "Q-values: {'up': 6.327182292099465, 'down': 7.95325847460904, 'left': 6.309244656066854, 'right': 7.829734568620287}\n",
      "Best action: down\n",
      "Action probabilities: {'up': 0.025, 'down': 0.925, 'left': 0.025, 'right': 0.025}\n",
      "\n",
      "State (2, 2): \n",
      "Q-values: {'up': 7.074136379748414, 'down': 8.926197351134867, 'left': 7.053192739585447, 'right': 8.797358713647874}\n",
      "Best action: down\n",
      "Action probabilities: {'up': 0.025, 'down': 0.925, 'left': 0.025, 'right': 0.025}\n",
      "\n",
      "State (2, 3): \n",
      "Q-values: {'up': 7.957312715181976, 'down': 8.91540195856801, 'left': 7.9357923629232605, 'right': 10.0}\n",
      "Best action: right\n",
      "Action probabilities: {'up': 0.025, 'down': 0.025, 'left': 0.025, 'right': 0.925}\n",
      "\n",
      "State (3, 0): \n",
      "Q-values: {'up': 7.0531115328409095, 'down': 7.826032849300737, 'left': 6.264780076406248, 'right': 6.995103836351358}\n",
      "Best action: down\n",
      "Action probabilities: {'up': 0.025, 'down': 0.925, 'left': 0.025, 'right': 0.025}\n",
      "\n",
      "State (3, 1): \n",
      "Q-values: {'up': 6.929634994289305, 'down': 8.922068660790922, 'left': 7.067695086216842, 'right': 7.765058769230784}\n",
      "Best action: down\n",
      "Action probabilities: {'up': 0.025, 'down': 0.925, 'left': 0.025, 'right': 0.025}\n",
      "\n",
      "State (3, 2): \n",
      "Q-values: {'up': 7.557571154649043, 'down': 10.0, 'left': 7.853362009953786, 'right': 8.88937431888217}\n",
      "Best action: down\n",
      "Action probabilities: {'up': 0.025, 'down': 0.925, 'left': 0.025, 'right': 0.025}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "iterations = 100000\n",
    "\n",
    "def policy(x,y,policy_dict):\n",
    "    state = (x,y)\n",
    "    return policy_dict[state]\n",
    "\n",
    "def MonteCarloControl(world,policy, iterations):\n",
    "    # initialisere\n",
    "    states = [(x, y) for x in range(world.width) for y in range(world.height)]\n",
    "    Q = {state: {action: 0 for action in ACTIONS} for state in states}\n",
    "    Returns = {state: {action: [] for action in ACTIONS} for state in states}\n",
    "    policy_dict = {state: {action: 1 / len(ACTIONS) for action in ACTIONS} for state in states}\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        episode = generate_episode(world, lambda x,y: policy(x,y, policy_dict), (0, 0))\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        \n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            \n",
    "            if (state, action) not in visited:\n",
    "                visited.add((state, action))\n",
    "                Returns[state][action].append(G)\n",
    "                Q[state][action] = sum(Returns[state][action]) / len(Returns[state][action])\n",
    "                max_action = max(Q[state], key=Q[state].get)\n",
    "                \n",
    "                for a in ACTIONS:\n",
    "                    if a == max_action:\n",
    "                        policy_dict[state][a] = 1 - epsilon + epsilon / len(ACTIONS)\n",
    "                    else:\n",
    "                        policy_dict[state][a] = epsilon / len(ACTIONS)\n",
    "    \n",
    "    return Q, policy_dict\n",
    "\n",
    "Q, learned_policy = MonteCarloControl(world, policy, iterations)\n",
    "for state in Q:\n",
    "    if not world.is_terminal(state[0],state[1]):\n",
    "        print(f\"State {state}: \")\n",
    "        print(f\"Q-values: {Q[state]}\")\n",
    "        print(f\"Best action: {max(learned_policy[state], key=learned_policy[state].get)}\")\n",
    "        print(f\"Action probabilities: {learned_policy[state]}\")\n",
    "        print()    \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
