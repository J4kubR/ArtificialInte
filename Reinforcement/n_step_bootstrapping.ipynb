{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8kdEPNmFOCr"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "T9cAvA0GLkXh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys \n",
    "import pandas as pd \n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wMAd6qTASn9u"
   },
   "outputs": [],
   "source": [
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\") \n",
    "REWARDS = {\" \": -1, \".\": 0.1, \"+\": 100, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\")\n",
    "OBSTACLES = (\"#\")\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "#Made here a random move possible if needed for other implementations, and for fun :)\n",
    "rand_move_probability = 0\n",
    "\n",
    "class World:  \n",
    "  def __init__(self, width, height):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.grid = np.full((width, height), ' ', dtype='U1')\n",
    "  \n",
    "  def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "    \"\"\"\n",
    "    Create an obstacle in either a single cell or rectangle.\n",
    "    \"\"\"\n",
    "    if end_x == None: end_x = start_x\n",
    "    if end_y == None: end_y = start_y\n",
    "    \n",
    "    self.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "  def add_reward(self, x, y, reward):\n",
    "    assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "    self.grid[x, y] = reward\n",
    "\n",
    "  def add_terminal(self, x, y, terminal):\n",
    "    assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "    self.grid[x, y] = terminal\n",
    "\n",
    "  def is_obstacle(self, x, y):\n",
    "    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "      return True\n",
    "    else:\n",
    "      return self.grid[x ,y] in OBSTACLES \n",
    "\n",
    "  def is_terminal(self, x, y):\n",
    "    return self.grid[x ,y] in TERMINALS\n",
    "\n",
    "  def get_reward(self, x, y):\n",
    "    \"\"\" \n",
    "    Return the reward associated with a given location\n",
    "    \"\"\" \n",
    "    return REWARDS[self.grid[x, y]]\n",
    "\n",
    "  def get_next_state(self, current_state, action):\n",
    "    \"\"\"\n",
    "    Get the next state given a current state and an action. The outcome can be\n",
    "    stochastic  where rand_move_probability determines the probability of \n",
    "    ignoring the action and performing a random move.\n",
    "    \"\"\"    \n",
    "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "    \n",
    "    x, y = current_state \n",
    "    \n",
    "    # If our current state is a terminal, there is no next state\n",
    "    if self.grid[x, y] in TERMINALS:\n",
    "      return None\n",
    "\n",
    "    # Check of a random action should be performed:\n",
    "    if np.random.rand() < rand_move_probability:\n",
    "      action = np.random.choice(ACTIONS)\n",
    "\n",
    "    if action == \"up\":      y -= 1\n",
    "    elif action == \"down\":  y += 1\n",
    "    elif action == \"left\":  x -= 1\n",
    "    elif action == \"right\": x += 1\n",
    "\n",
    "    # If the next state is an obstacle, stay in the current state\n",
    "    return (x, y) if not self.is_obstacle(x, y) else current_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVpq3vosHAol",
    "outputId": "c29365e2-2dea-4772-907d-af80719ffa37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' '+']]\n"
     ]
    }
   ],
   "source": [
    "world = World(4, 4)\n",
    "world.add_terminal(3, 3, \"+\")\n",
    "\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of on-policy n-step Sarsa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MsdFod-Xx5iY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def n_step_sarsa(world, n, alpha, epsilon, gamma, num_episodes = 1000):\n",
    "    states = [(x, y) for x in range(world.width) for y in range(world.height)]\n",
    "    Q = {state: {action: 0 for action in ACTIONS} for state in states}\n",
    "    policy_dict = {state: {action: 1 / len(ACTIONS) for action in ACTIONS} for state in states}\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state = (np.random.randint(0, world.width), np.random.randint(0, world.height))\n",
    "        while world.is_terminal(*state):\n",
    "            state = (np.random.randint(0, world.width), np.random.randint(0, world.height))\n",
    "        action = epsilon_greedy_policy(state, policy_dict, epsilon)\n",
    "        \n",
    "        T = float('inf')\n",
    "        t = 0\n",
    "        G = 0\n",
    "\n",
    "        states_list = [state]\n",
    "        actions_list = [action]\n",
    "        rewards_list = [0]\n",
    "\n",
    "        while True:\n",
    "            if t < T:\n",
    "                next_state = world.get_next_state(state, action)\n",
    "                reward = world.get_reward(*state)\n",
    "                rewards_list.append(reward)\n",
    "                next_action = \"None\"\n",
    "\n",
    "                if next_state is None or world.is_terminal(*next_state):\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    next_action = epsilon_greedy_policy(next_state,policy_dict, epsilon)\n",
    "                    states_list.append(next_state)\n",
    "                    actions_list.append(next_action)\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                G = sum(gamma ** (i - tau - 1) * rewards_list[i] for i in range(tau + 1, min(tau + n, T) + 1))\n",
    "                \n",
    "                if tau + n < T:\n",
    "                    G += gamma ** n * Q[states_list[tau + n]][actions_list[tau + n]]\n",
    "\n",
    "                current_state = states_list[tau]\n",
    "                current_action = actions_list[tau]\n",
    "                Q[current_state][current_action] += alpha * (G - Q[current_state][current_action])\n",
    "\n",
    "            if tau == T - 1: \n",
    "                break\n",
    "            t += 1\n",
    "\n",
    "        for state in states:\n",
    "            action_values = Q[state]\n",
    "            best_action = max(action_values, key=action_values.get)\n",
    "            for action in ACTIONS:\n",
    "                policy_dict[state][action] = epsilon / len(ACTIONS)\n",
    "            policy_dict[state][best_action] += 1 - epsilon\n",
    "\n",
    "    return Q, policy_dict\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, policy_dict, epsilon=0.1):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        action_values = policy_dict[state]\n",
    "        max_prob = max(action_values.values())\n",
    "        best_actions = [action for action, prob in action_values.items() if prob == max_prob]\n",
    "        return random.choice(best_actions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(Q, policy, world=world, method='Q'):\n",
    "    grid_width = world.width\n",
    "    grid_height = world.height\n",
    "\n",
    "    grid = [['' for _ in range(grid_width)] for _ in range(grid_height)]\n",
    "    \n",
    "    for state in sorted(Q.keys()):\n",
    "\n",
    "        if world.is_terminal(*state):\n",
    "            continue\n",
    "        \n",
    "        if method == 'Q':\n",
    "            best_action = max(Q[state], key=Q[state].get)\n",
    "        elif method == 'policy':\n",
    "            best_action = max(policy[state], key=policy[state].get)\n",
    "        else:\n",
    "            raise ValueError(\"Method should be either 'Q' or 'policy'\")\n",
    "        \n",
    "        action_abbr = action_abbreviation(best_action)\n",
    "        \n",
    "        x, y = state\n",
    "\n",
    "        grid[y][x] = action_abbr\n",
    "    \n",
    "    for row in grid:\n",
    "        print(\" \".join(row))\n",
    "\n",
    "def action_abbreviation(action):\n",
    "    if action == 'up':\n",
    "        return 'U'\n",
    "    elif action == 'down':\n",
    "        return 'D'\n",
    "    elif action == 'left':\n",
    "        return 'L'\n",
    "    elif action == 'right':\n",
    "        return 'R'\n",
    "    else:\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R D R D\n",
      "D R D D\n",
      "R R D D\n",
      "R R R \n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "gamma = 0.99\n",
    "num_episodes = 10000\n",
    "\n",
    "Q, policy = n_step_sarsa(world, n, alpha, epsilon, gamma, num_episodes)\n",
    "print_policy(Q,policy, method='Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) right\n",
      "(0, 1) down\n",
      "(0, 2) right\n",
      "(0, 3) right\n",
      "(1, 0) down\n",
      "(1, 1) right\n",
      "(1, 2) right\n",
      "(1, 3) right\n",
      "(2, 0) right\n",
      "(2, 1) down\n",
      "(2, 2) down\n",
      "(2, 3) right\n",
      "(3, 0) down\n",
      "(3, 1) down\n",
      "(3, 2) down\n",
      "(3, 3) up\n"
     ]
    }
   ],
   "source": [
    "for item in Q.keys():\n",
    "    print(item,max(Q[item],key=Q[item].get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 0), {'up': -33.175794607787175, 'down': -31.185526022476147, 'left': -17.659041987538437, 'right': -6.894798500186999})\n",
      "((0, 1), {'up': -10.893573012228329, 'down': -5.714833566381222, 'left': -11.37883214789113, 'right': -12.704090892627159})\n",
      "((0, 2), {'up': -8.844059022094493, 'down': -4.9374884393423, 'left': -6.321997649674107, 'right': -4.319578369492129})\n",
      "((0, 3), {'up': -7.135620281116557, 'down': -9.349098911200908, 'left': -8.602590246937837, 'right': -3.094968702374016})\n",
      "((1, 0), {'up': -8.441468023550861, 'down': -5.1301533131843735, 'left': -10.01677788540369, 'right': -8.435440704124549})\n",
      "((1, 1), {'up': -6.191701621919399, 'down': -5.105445962199412, 'left': -6.655227536188954, 'right': -4.15123400437261})\n",
      "((1, 2), {'up': -5.193985662167155, 'down': -4.206290506404372, 'left': -5.5751156079189, 'right': -3.057710192381763})\n",
      "((1, 3), {'up': -4.740462068229153, 'down': -3.3634678069411614, 'left': -4.540297097003025, 'right': -2.060643107256684})\n",
      "((2, 0), {'up': -15.239742493693601, 'down': -14.734701635071998, 'left': -21.952366341045206, 'right': -4.126961732479809})\n",
      "((2, 1), {'up': -15.638105746443054, 'down': -3.0678440199409946, 'left': -10.107522871675203, 'right': -9.994197207333798})\n",
      "((2, 2), {'up': -4.339728835683297, 'down': -2.0561622675312, 'left': -4.27241973331434, 'right': -2.3518154339407094})\n",
      "((2, 3), {'up': -3.318854977802057, 'down': -2.18185578228763, 'left': -3.2645343404875953, 'right': -0.9999999999999996})\n",
      "((3, 0), {'up': -6.916381459598913, 'down': -3.089213199044835, 'left': -8.979205109022358, 'right': -6.865129550878742})\n",
      "((3, 1), {'up': -4.8075032034135905, 'down': -2.2438740016805068, 'left': -4.756126119613344, 'right': -3.4905023426861304})\n",
      "((3, 2), {'up': -3.153072958574197, 'down': -0.9999999999999996, 'left': -3.116812034348257, 'right': -2.3267260364912725})\n",
      "((3, 3), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n"
     ]
    }
   ],
   "source": [
    "for item in Q.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
